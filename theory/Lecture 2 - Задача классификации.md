
Встречается чаще, чем регрессия 

Состоит из предсказания дискретного значения, в противоположность регрессии 

Для классификации в датасете должны быть проведены метки классов для каждого объекта, то есть датасет должен быть "размечен"

если заранее нет информации о классах, то это уже совсем другая задача - задача кластеризации, которая относится к обучению без учителя


Классификация может быть бинарной или множественной, одноклассовой (1 объект принадлежит строго 1 классу) и многоклассовой (1 объект может принадлежать нескольким классам)

например перевод текста - задачи классификации, потому что мы, исходя из контекста, выбираем следующее слово хоть и из большого количества вариантов, но все-так из конечного 

# Логистическая регрессия

назван так по историческим причинам и является подходом НЕ К РЕРГЕРССИОННЫМ, а к КЛАССИФИКАЦИОННЫМ задачам


Использование нелинейного преобразования, например такого:

$$
\frac{1}{1 + e^{-z}}
$$

где z - результат работы модели линейной регрессии, который передается в нелинейное преобразование, которое ограничивает область значений сверху и снизу


Граница принятия решений - это линия, которая разделяет области разных классов. 

Так как мы используем ЛИНЕЙНУЮ функцию ВНУТРИ логистической, то граница принятия решений - прямая, плоскость или гиперплоскость 


ДАННЫЕ могут обладать свойством линейной разделимости, тогда модель логистической регрессии применима



Многоклассовая классификация: Один против всех

последовательно выделяется один класс среди всех остальных
по сути задача делится на N+1 задачу бинарной классификации 

затем модель для каждого объекта выдает вектор вероятностей, выбирается класс, чья вероятность выше остальных

# Нормализация данных

градиентный спуск - поиск локального экстремума функции ошибки (поиск минимальной ошибки) 

если данные не нормализованы, то функция ошибки (парабалоид при 2 параметрах) получается очень вытянута, соответсвенно градиентный спуск не доходит на минимума


MinMaxScaler
$$x^i \ = \frac{x-x_{min}}{x_{max}-x_{min}}$$

StandardScaler
$$x^i \ = \frac{x-M[x]}{\sigma_{x}}$$



